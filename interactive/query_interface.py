#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Simple 4-Stage Query Pipeline - Following README Requirements Exactly
Implements: Simple, Readable, Maintainable with DRY, SOLID, YAGNI
"""

import asyncio
import json
import re
import pyodbc
import time
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

# SQLGlot for AST validation - README requirement
try:
    import sqlglot
    HAS_SQLGLOT = True
except ImportError:
    HAS_SQLGLOT = False
    print("‚ö†Ô∏è SQLGlot not available - install with: pip install sqlglot")

from langchain_openai import AzureChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

from shared.config import Config
from shared.models import TableInfo, BusinessDomain, Relationship, QueryResult
from shared.utils import parse_json_response, clean_sql_query, safe_database_value

class SimpleRAGClient:
    """Simple LLM client - Single responsibility"""
    
    def __init__(self, config: Config):
        self.llm = AzureChatOpenAI(
            azure_endpoint=config.azure_endpoint,
            api_key=config.api_key,
            azure_deployment=config.deployment_name,
            api_version=config.api_version,
            request_timeout=60,
            temperature=1.0  # Use default temperature to avoid API errors
        )
    
    async def analyze(self, system_prompt: str, user_prompt: str) -> str:
        """Simple LLM analysis"""
        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            response = await asyncio.to_thread(self.llm.invoke, messages)
            return response.content
        except Exception as e:
            print(f"   ‚ö†Ô∏è LLM error: {e}")
            return ""

class TableSelector:
    """Stage 2: Explainable table selection - README Pattern B"""
    
    def __init__(self, tables: List[TableInfo]):
        self.tables = tables
    
    async def select_relevant_tables(self, question: str, llm: SimpleRAGClient) -> Tuple[List[TableInfo], Dict]:
        """Select tables with explainable reasoning"""
        print("   üìã Stage 2: Explainable table selection...")
        
        explanations = {
            'total_candidates': len(self.tables),
            'lexical_matches': [],
            'selected_tables': [],
            'reasoning': []
        }
        
        # Step 1: Quick lexical filtering
        candidates = self.lexical_filter(question, explanations)
        
        # Step 2: LLM-based selection if too many candidates
        if len(candidates) > 8:
            selected = await self.llm_selection(question, candidates, llm, explanations)
        else:
            selected = candidates
            explanations['reasoning'].append(f"Used all {len(candidates)} lexically matched tables")
        
        explanations['selected_tables'] = [t.full_name for t in selected]
        
        print(f"      ‚úÖ Selected {len(selected)} tables from {len(candidates)} candidates")
        for reason in explanations['reasoning'][-2:]:
            print(f"      ‚Ä¢ {reason}")
        
        return selected, explanations
    
    def lexical_filter(self, question: str, explanations: Dict) -> List[TableInfo]:
        """Enhanced lexical matching with better filtering"""
        q_lower = question.lower()
        question_words = [w for w in q_lower.split() if len(w) > 2]
        
        scored_tables = []
        total_matches = 0
        
        for table in self.tables:
            score = 0.0
            reasons = []
            
            # Table name matching (higher weight)
            table_name_lower = table.name.lower()
            name_matches = [word for word in question_words if word in table_name_lower]
            if name_matches:
                score += len(name_matches) * 3.0
                reasons.append(f"name_match:{','.join(name_matches)}")
            
            # Entity type matching with specific keywords
            entity_mapping = {
                'customer': ['customer', 'client', 'user', 'account', 'contact'],
                'payment': ['payment', 'paid', 'billing', 'invoice', 'revenue', 'financial', 'collection'],
                'order': ['order', 'sale', 'purchase', 'transaction', 'contract'],
                'financial': ['revenue', 'amount', 'total', 'value', 'price', 'cost']
            }
            
            table_entity = table.entity_type.lower()
            for entity, keywords in entity_mapping.items():
                if any(kw in q_lower for kw in keywords):
                    if entity in table_entity or any(kw in table_name_lower for kw in keywords):
                        score += 2.0
                        reasons.append(f"entity_match:{entity}")
            
            # Column matching (specific to question intent)
            column_names = [col.get('name', '').lower() for col in table.columns]
            relevant_columns = []
            
            # Look for payment-related columns
            if any(w in q_lower for w in ['paid', 'payment', 'revenue']):
                payment_cols = [col for col in column_names if any(p in col for p in ['payment', 'amount', 'paid', 'revenue', 'invoice'])]
                if payment_cols:
                    score += 2.0
                    relevant_columns.extend(payment_cols[:3])
                    reasons.append(f"payment_columns:{','.join(payment_cols[:3])}")
            
            # Look for date columns (important for "2025" filtering)
            if any(w in q_lower for w in ['2025', '2024', 'year']):
                date_cols = [col for col in column_names if any(d in col for d in ['date', 'time', 'year', 'created', 'modified'])]
                if date_cols:
                    score += 1.5
                    relevant_columns.extend(date_cols[:3])
                    reasons.append(f"date_columns:{','.join(date_cols[:3])}")
            
            # Customer-related columns
            if any(w in q_lower for w in ['customer', 'client']):
                customer_cols = [col for col in column_names if any(c in col for c in ['customer', 'client', 'user', 'contact'])]
                if customer_cols:
                    score += 1.5
                    relevant_columns.extend(customer_cols[:3])
                    reasons.append(f"customer_columns:{','.join(customer_cols[:3])}")
            
            # Data availability bonus
            if table.row_count > 0:
                score += 0.5
                reasons.append(f"has_data:{table.row_count:,}")
            
            # Business role bonus
            if table.business_role == 'Core':
                score += 1.0
                reasons.append("core_business_table")
            
            if score > 0:
                total_matches += 1
                scored_tables.append((table, score, reasons))
                explanations['lexical_matches'].append({
                    'table': table.full_name,
                    'entity_type': table.entity_type,
                    'score': score,
                    'reasoning': '; '.join(reasons)
                })
        
        # Sort by score and return meaningful candidates
        scored_tables.sort(key=lambda x: x[1], reverse=True)
        
        # Dynamic candidate selection based on score distribution
        if len(scored_tables) > 20:
            # Take top performers with score > 2.0
            candidates = [table for table, score, _ in scored_tables if score > 2.0][:15]
        else:
            candidates = [table for table, _, _ in scored_tables[:15]]
        
        explanations['reasoning'].append(f"Lexical filtering: {len(candidates)} high-relevance tables from {total_matches} matches")
        return candidates
    
    async def llm_selection(self, question: str, candidates: List[TableInfo], 
                          llm: SimpleRAGClient, explanations: Dict) -> List[TableInfo]:
        """LLM-based table selection"""
        
        # Prepare candidate summaries
        table_summaries = []
        for table in candidates:
            table_summaries.append({
                'table_name': table.full_name,
                'entity_type': table.entity_type,
                'row_count': table.row_count,
                'columns': [col.get('name') for col in table.columns[:5]]
            })
        
        system_prompt = """You are a database analyst. Select the most relevant tables for the user question.
Focus on tables that directly relate to what the user is asking."""
        
        user_prompt = f"""
QUESTION: "{question}"

CANDIDATE TABLES:
{json.dumps(table_summaries, indent=2)}

Select the 6 most relevant tables. Respond with JSON:
{{
  "selected_tables": ["[schema].[table1]", "[schema].[table2]"],
  "reasoning": "Why these tables are most relevant"
}}
"""
        
        response = await llm.analyze(system_prompt, user_prompt)
        result = parse_json_response(response)
        
        if result and 'selected_tables' in result:
            selected_names = result['selected_tables']
            selected = [t for t in candidates if t.full_name in selected_names]
            explanations['reasoning'].append(f"LLM selection: {result.get('reasoning', 'Semantic relevance')}")
            return selected
        
        # Fallback to top scored tables
        explanations['reasoning'].append("LLM selection failed, using top scored tables")
        return candidates[:6]

class SQLGenerator:
    """Stage 4: Constrained SQL generation - README Pattern A"""
    
    def __init__(self, config: Config):
        self.config = config
        self.database_structure = self.load_database_structure()
        self.view_patterns = self.extract_view_patterns()
    
    def load_database_structure(self) -> Dict:
        """Load database structure for constraints"""
        try:
            structure_file = self.config.get_cache_path("database_structure.json")
            if structure_file.exists():
                with open(structure_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception:
            pass
        return {}
    
    def extract_view_patterns(self) -> List[Dict]:
        """Extract proven join patterns from views - README requirement"""
        patterns = []
        views = self.database_structure.get('view_info', {})
        
        for view_name, view_data in views.items():
            if view_data.get('definition') and view_data.get('execution_success'):
                # Extract business pattern information
                business_pattern = view_data.get('business_pattern', {})
                
                patterns.append({
                    'view_name': view_name,
                    'definition': view_data['definition'],
                    'business_pattern': business_pattern.get('pattern', 'unknown'),
                    'use_case': business_pattern.get('estimated_use_case', ''),
                    'sample_data': view_data.get('sample_data', []),
                    'columns': view_data.get('columns_returned', []),
                    'proven_working': True,
                    'confidence': business_pattern.get('confidence', 0.0)
                })
        
        print(f"      üìä Loaded {len(patterns)} proven view patterns for business logic")
        return patterns
    
    async def generate_sql(self, question: str, tables: List[TableInfo], 
                          explanations: Dict, llm: SimpleRAGClient) -> str:
        """Generate constrained SQL with view patterns and SQLGlot validation - README requirement"""
        print("   ‚ö° Stage 4: Constrained SQL generation with view patterns & SQLGlot...")
        
        # Analyze intent with better detection
        intent = self.analyze_intent(question)
        
        # Find relevant view patterns for this question
        relevant_patterns = self.find_relevant_view_patterns(question, tables)
        
        if relevant_patterns:
            print(f"      üìã Found {len(relevant_patterns)} relevant view patterns")
            # Try to generate SQL using proven view patterns first
            sql = await self.generate_using_view_patterns(question, tables, relevant_patterns, intent, llm)
            
            # Validate with SQLGlot
            is_valid, validation_msg, ast = self.validate_sql_with_sqlglot(sql)
            
            if is_valid:
                print(f"      ‚úÖ Generated SQL using view patterns (SQLGlot validated)")
                return sql
            else:
                print(f"      ‚ö†Ô∏è View pattern SQL failed validation: {validation_msg}")
        
        # Fallback to table-based generation
        print(f"      üîÑ Generating SQL using table analysis...")
        context = self.build_enhanced_context(tables, intent, relevant_patterns)
        sql = await self.generate_table_based_sql(question, context, intent, llm)
        
        # Validate and enhance
        sql = self.enhance_sql_with_intent(sql, intent, tables)
        
        # Final SQLGlot validation
        is_valid, validation_msg, ast = self.validate_sql_with_sqlglot(sql)
        
        if not is_valid:
            print(f"      ‚ö†Ô∏è Generated SQL failed SQLGlot validation: {validation_msg}")
            sql = self.generate_safe_fallback_sql(tables, intent)
            print(f"      ‚úÖ Using safe fallback SQL")
        else:
            print(f"      ‚úÖ Generated SQL passed SQLGlot validation")
        
        return sql
    
    async def generate_table_based_sql(self, question: str, context: str, 
                                     intent: Dict, llm: SimpleRAGClient) -> str:
        """Generate SQL using table analysis when view patterns aren't applicable"""
        
        system_prompt = f"""You are an expert SQL generator with access to proven business patterns from database views.

CRITICAL REQUIREMENTS:
1. Use EXACT table and column names from the context
2. Include proper date filtering when specified (e.g., YEAR(column) = {intent.get('date_year', 2025)} for year filtering)
3. Use proven join patterns when multiple tables are needed
4. Return only SELECT statements with appropriate TOP limits
5. Apply business logic from available patterns

BUSINESS CONTEXT:
{context}"""
        
        user_prompt = f"""
QUESTION: "{question}"

INTENT ANALYSIS:
- Aggregation: {intent.get('aggregation', 'none')}
- Entity Focus: {intent.get('entity_focus', 'general')}
- Date Filter Required: {intent.get('has_date_filter', False)}
- Year Filter: {intent.get('date_year', 'none')}
- Business Context: {intent.get('business_context', 'general')}

Generate SQL that:
1. Answers the question precisely
2. Includes date filtering for year {intent.get('date_year')} if required
3. Uses available business patterns when applicable
4. Joins tables when business logic requires it
5. Applies proper aggregation if needed

CRITICAL: If the question asks for year {intent.get('date_year')}, you MUST include a date filter like:
WHERE YEAR(date_column) = {intent.get('date_year')}

Return only the SQL query:
"""
        
        response = await llm.analyze(system_prompt, user_prompt)
        return clean_sql_query(response)
    
    async def generate_using_view_patterns(self, question: str, tables: List[TableInfo], 
                                         patterns: List[Dict], intent: Dict, llm: SimpleRAGClient) -> str:
        """Generate SQL by adapting proven view patterns - README requirement"""
        
        # Find the best matching view pattern
        best_pattern = patterns[0]  # Already sorted by relevance
        
        # Extract the pattern's business logic
        view_definition = best_pattern['definition']
        business_pattern = best_pattern['business_pattern']
        proven_joins = best_pattern.get('proven_joins', [])
        
        system_prompt = f"""You are an expert SQL generator using PROVEN business patterns from database views.

CRITICAL: You must adapt the proven view pattern below to answer the user's question.

PROVEN VIEW PATTERN:
View: {best_pattern['view_name']}
Business Pattern: {business_pattern}
Use Case: {best_pattern['use_case']}
Confidence: {best_pattern['confidence']:.1f}

PROVEN SQL DEFINITION:
{view_definition}

PROVEN JOIN PATTERNS:
{json.dumps(proven_joins, indent=2)}

AVAILABLE TABLES:
{self.format_tables_for_pattern(tables)}

CONSTRAINTS:
1. Use the PROVEN join logic from the view definition above
2. Adapt the SELECT and WHERE clauses for the user's question
3. Use EXACT table and column names from available tables
4. Include proper date filtering when specified
5. Return only SELECT statements"""
        
        user_prompt = f"""
USER QUESTION: "{question}"

INTENT:
- Aggregation: {intent.get('aggregation', 'none')}
- Date Filter: {intent.get('date_year', 'none')} 
- Entity Focus: {intent.get('entity_focus', 'general')}

TASK: Adapt the proven view pattern to answer this question.

Steps:
1. Use the proven JOIN logic from the view definition
2. Modify SELECT clause for the aggregation needed
3. Add WHERE clause for date filtering if needed: YEAR(date_column) = {intent.get('date_year', 2025)}
4. Use the business logic from the proven pattern

Generate adapted SQL:
"""
        
        response = await llm.analyze(system_prompt, user_prompt)
        return clean_sql_query(response)
    
    def format_tables_for_pattern(self, tables: List[TableInfo]) -> str:
        """Format tables for pattern-based generation"""
        formatted = []
        for table in tables:
            formatted.append(f"TABLE: {table.full_name}")
            formatted.append(f"  Entity: {table.entity_type}")
            
            # Focus on key columns
            key_columns = []
            for col in table.columns[:8]:
                col_name = col.get('name', '')
                col_type = col.get('data_type', '')
                key_columns.append(f"{col_name} ({col_type})")
            
            formatted.append(f"  Columns: {', '.join(key_columns)}")
        
        return '\n'.join(formatted)
    
    def generate_safe_fallback_sql(self, tables: List[TableInfo], intent: Dict) -> str:
        """Generate guaranteed safe SQL that will pass validation"""
        if not tables:
            return ""
        
        # Use the table with most data
        table = max(tables, key=lambda t: t.row_count)
        
        # Find safe columns (avoid complex data types)
        safe_columns = []
        date_columns = []
        
        for col in table.columns:
            col_name = col.get('name', '')
            col_type = col.get('data_type', '').lower()
            col_lower = col_name.lower()
            
            # Identify date columns
            if any(d in col_lower for d in ['date', 'time', 'created', 'modified']):
                date_columns.append(col_name)
            
            # Safe column types
            if any(safe_type in col_type for safe_type in ['int', 'varchar', 'nvarchar', 'char', 'bit', 'datetime']):
                safe_columns.append(col_name)
        
        # Build safe query
        if intent.get('aggregation') == 'count':
            if intent.get('entity_focus') == 'customer':
                # Count distinct customers
                customer_cols = [col for col in safe_columns if 'customer' in col.lower()]
                if customer_cols:
                    count_column = customer_cols[0]
                    sql = f"SELECT COUNT(DISTINCT [{count_column}]) as paid_customers FROM {table.full_name}"
                else:
                    sql = f"SELECT COUNT(*) as total_count FROM {table.full_name}"
            else:
                sql = f"SELECT COUNT(*) as total_count FROM {table.full_name}"
        else:
            # Select top records
            limit = min(intent.get('limit', 100), 1000)  # Safety limit
            if safe_columns:
                columns_str = ', '.join([f"[{col}]" for col in safe_columns[:5]])
                sql = f"SELECT TOP {limit} {columns_str} FROM {table.full_name}"
            else:
                sql = f"SELECT TOP {limit} * FROM {table.full_name}"
        
        # Add date filtering if required and available
        if intent.get('has_date_filter') and intent.get('date_year') and date_columns:
            year = intent['date_year']
            date_col = date_columns[0]
            sql += f" WHERE YEAR([{date_col}]) = {year}"
        
        return sql
    
    def find_relevant_view_patterns(self, question: str, tables: List[TableInfo]) -> List[Dict]:
        """Find view patterns relevant to the question - README requirement"""
        if not self.view_patterns:
            return []
        
        q_lower = question.lower()
        table_names = [t.full_name.lower() for t in tables]
        relevant = []
        
        for pattern in self.view_patterns:
            relevance_score = 0.0
            
            # Business pattern matching
            business_pattern = pattern.get('business_pattern', '')
            if any(keyword in q_lower for keyword in ['customer', 'payment', 'paid']):
                if any(bp in business_pattern for bp in ['customer', 'payment', 'financial']):
                    relevance_score += 3.0
            
            # Use case matching
            use_case = pattern.get('use_case', '').lower()
            if 'payment' in q_lower and 'payment' in use_case:
                relevance_score += 2.0
            if 'customer' in q_lower and 'customer' in use_case:
                relevance_score += 2.0
            
            # Table involvement - check if pattern uses our selected tables
            definition = pattern.get('definition', '').lower()
            table_overlap = sum(1 for table_name in table_names if any(part in definition for part in table_name.split('.')))
            relevance_score += table_overlap * 1.0
            
            # Confidence bonus
            relevance_score += pattern.get('confidence', 0.0) * 0.5
            
            if relevance_score > 1.0:
                pattern['relevance_score'] = relevance_score
                relevant.append(pattern)
        
        # Sort by relevance and return top patterns
        relevant.sort(key=lambda x: x['relevance_score'], reverse=True)
        return relevant[:3]
    
    def build_enhanced_context(self, tables: List[TableInfo], intent: Dict, patterns: List[Dict]) -> str:
        """Build enhanced context with view patterns"""
        context = ["AVAILABLE TABLES AND PROVEN PATTERNS:"]
        
        # Add table information
        for table in tables:
            context.append(f"\nTABLE: {table.full_name}")
            context.append(f"  Entity: {table.entity_type}")
            context.append(f"  Business Role: {table.business_role}")
            context.append(f"  Rows: {table.row_count:,}")
            
            # Categorize columns by type
            date_columns = []
            customer_columns = []
            amount_columns = []
            other_columns = []
            
            for col in table.columns[:12]:
                col_name = col.get('name', '')
                col_type = col.get('data_type', '')
                col_lower = col_name.lower()
                
                if any(d in col_lower for d in ['date', 'time', 'created', 'modified', 'year']):
                    date_columns.append(f"{col_name} ({col_type})")
                elif any(c in col_lower for c in ['customer', 'client', 'user', 'contact']):
                    customer_columns.append(f"{col_name} ({col_type})")
                elif any(a in col_lower for a in ['amount', 'value', 'price', 'cost', 'revenue', 'payment']):
                    amount_columns.append(f"{col_name} ({col_type})")
                else:
                    other_columns.append(f"{col_name} ({col_type})")
            
            if date_columns:
                context.append(f"  üìÖ Date Columns: {', '.join(date_columns)}")
            if customer_columns:
                context.append(f"  üë§ Customer Columns: {', '.join(customer_columns)}")
            if amount_columns:
                context.append(f"  üí∞ Amount Columns: {', '.join(amount_columns)}")
            if other_columns:
                context.append(f"  üìã Other Columns: {', '.join(other_columns[:5])}")
            
            # Add sample data with business context
            if table.sample_data:
                sample = table.sample_data[0]
                # Focus on relevant sample data
                relevant_sample = {}
                for key, value in sample.items():
                    if any(keyword in key.lower() for keyword in ['customer', 'date', 'amount', 'payment', 'revenue']):
                        relevant_sample[key] = value
                        if len(relevant_sample) >= 3:
                            break
                
                if relevant_sample:
                    sample_str = ', '.join([f"{k}: {v}" for k, v in relevant_sample.items()])
                    context.append(f"  üí° Sample: {sample_str}")
        
        # Add proven view patterns
        if patterns:
            context.append("\nüéØ PROVEN BUSINESS PATTERNS FROM VIEWS:")
            for pattern in patterns:
                context.append(f"\n  View: {pattern['view_name']}")
                context.append(f"  Pattern: {pattern['business_pattern']}")
                context.append(f"  Use Case: {pattern['use_case']}")
                context.append(f"  Confidence: {pattern.get('confidence', 0.0):.1f}")
                
                # Extract key join patterns from definition
                definition = pattern['definition']
                joins = re.findall(r'JOIN\s+\[?(\w+)\]?\.\[?(\w+)\]?\s+.*?ON\s+([^WHERE^GROUP^ORDER^INNER^LEFT^RIGHT^JOIN]+)', 
                                 definition, re.IGNORECASE)
                if joins:
                    context.append(f"  üí° Proven Joins: {len(joins)} join patterns available")
        
        # Add intent-specific guidance
        context.append(f"\nüéØ QUERY INTENT:")
        context.append(f"  Focus: {intent.get('entity_focus', 'general')}")
        context.append(f"  Aggregation: {intent.get('aggregation', 'none')}")
        context.append(f"  Date Filter: {'Required for ' + str(intent.get('date_year', 'current year')) if intent.get('has_date_filter') else 'None'}")
        context.append(f"  Limit: {intent.get('limit', 100)}")
        
        return '\n'.join(context)
    
    def analyze_intent(self, question: str) -> Dict:
        """Enhanced intent analysis"""
        q_lower = question.lower()
        
        intent = {
            'aggregation': None,
            'limit': 100,
            'has_date_filter': False,
            'date_year': None,
            'entity_focus': None,
            'requires_joins': False,
            'business_context': None
        }
        
        # Detect aggregation type
        if any(w in q_lower for w in ['how many', 'count', 'number of']):
            intent['aggregation'] = 'count'
        elif any(w in q_lower for w in ['total', 'sum', 'amount']):
            intent['aggregation'] = 'sum'
        elif any(w in q_lower for w in ['average', 'avg', 'mean']):
            intent['aggregation'] = 'avg'
        
        # Extract specific limits
        limit_patterns = [
            r'top\s+(\d+)', r'first\s+(\d+)', r'(\d+)\s+(?:customers|records|rows)',
            r'limit\s+(\d+)', r'(\d+)\s+most', r'(\d+)\s+best'
        ]
        for pattern in limit_patterns:
            match = re.search(pattern, q_lower)
            if match:
                intent['limit'] = int(match.group(1))
                break
        
        # Detect date filtering requirements
        year_match = re.search(r'(20\d{2})', question)
        if year_match:
            intent['has_date_filter'] = True
            intent['date_year'] = int(year_match.group(1))
        elif any(w in q_lower for w in ['this year', 'current year', 'recent', 'latest']):
            intent['has_date_filter'] = True
            intent['date_year'] = 2025  # Current context year
        
        # Determine entity focus
        if any(w in q_lower for w in ['customer', 'client', 'user']):
            intent['entity_focus'] = 'customer'
        elif any(w in q_lower for w in ['payment', 'paid', 'revenue', 'billing']):
            intent['entity_focus'] = 'payment'
        elif any(w in q_lower for w in ['order', 'sale', 'purchase', 'transaction']):
            intent['entity_focus'] = 'order'
        
        # Detect business context
        if any(w in q_lower for w in ['paid', 'payment']):
            intent['business_context'] = 'payment_analysis'
            intent['requires_joins'] = True  # Likely need customer + payment data
        elif any(w in q_lower for w in ['revenue', 'income']):
            intent['business_context'] = 'financial_analysis'
            intent['requires_joins'] = True
        elif any(w in q_lower for w in ['customer', 'client']):
            intent['business_context'] = 'customer_analysis'
        
        return intent
    
    def build_context(self, tables: List[TableInfo], intent: Dict) -> str:
        """Build context with view patterns"""
        context = ["AVAILABLE TABLES:"]
        
        for table in tables:
            context.append(f"\nTABLE: {table.full_name}")
            context.append(f"  Entity: {table.entity_type}")
            context.append(f"  Rows: {table.row_count:,}")
            
            columns = [f"{col.get('name')} ({col.get('data_type', 'unknown')})" 
                      for col in table.columns[:8]]
            context.append(f"  Columns: {', '.join(columns)}")
            
            # Add sample data preview
            if table.sample_data:
                sample = table.sample_data[0]
                sample_preview = ', '.join([f"{k}: {v}" for k, v in list(sample.items())[:3]])
                context.append(f"  Sample: {sample_preview}")
        
        # Add proven join patterns
        if self.view_patterns:
            context.append("\nPROVEN JOIN PATTERNS FROM VIEWS:")
            for pattern in self.view_patterns[:3]:
                context.append(f"  View: {pattern['view_name']} (proven working)")
                definition = pattern['definition'][:200] + "..." if len(pattern['definition']) > 200 else pattern['definition']
                context.append(f"  Pattern: {definition}")
        
        return '\n'.join(context)
    
    async def generate_with_view_patterns(self, question: str, context: str, 
                                        intent: Dict, llm: SimpleRAGClient) -> str:
        """Generate SQL using proven view patterns and business logic"""
        
        system_prompt = f"""You are an expert SQL generator with access to proven business patterns from database views.

CRITICAL REQUIREMENTS:
1. Use EXACT table and column names from the context
2. Include proper date filtering when specified (e.g., YEAR(column) = {intent.get('date_year', 2025)} for year filtering)
3. Use proven join patterns when multiple tables are needed
4. Return only SELECT statements with appropriate TOP limits
5. Apply business logic from proven view patterns

BUSINESS CONTEXT:
{context}"""
        
        user_prompt = f"""
QUESTION: "{question}"

INTENT ANALYSIS:
- Aggregation: {intent.get('aggregation', 'none')}
- Entity Focus: {intent.get('entity_focus', 'general')}
- Date Filter Required: {intent.get('has_date_filter', False)}
- Year Filter: {intent.get('date_year', 'none')}
- Business Context: {intent.get('business_context', 'general')}

Generate SQL that:
1. Answers the question precisely
2. Includes date filtering for year {intent.get('date_year')} if required
3. Uses proven business patterns when available
4. Joins tables when business logic requires it
5. Applies proper aggregation if needed

CRITICAL: If the question asks for year {intent.get('date_year')}, you MUST include a date filter like:
WHERE YEAR(date_column) = {intent.get('date_year')}

Return only the SQL query:
"""
        
        response = await llm.analyze(system_prompt, user_prompt)
        return clean_sql_query(response)
    
    def enhance_sql_with_intent(self, sql: str, intent: Dict, tables: List[TableInfo]) -> str:
        """Enhance SQL to ensure it matches the intent - especially date filtering"""
        if not sql:
            return sql
        
        # Critical fix: Add missing date filtering for year requests
        if intent.get('has_date_filter') and intent.get('date_year'):
            year = intent['date_year']
            
            # Check if SQL already has year filtering
            if f'YEAR(' not in sql.upper() and str(year) not in sql:
                # Find potential date columns in the tables
                date_columns = []
                for table in tables:
                    for col in table.columns:
                        col_name = col.get('name', '').lower()
                        if any(d in col_name for d in ['date', 'time', 'created', 'modified']):
                            date_columns.append((table.full_name, col.get('name')))
                
                if date_columns and 'WHERE' in sql.upper():
                    # Add to existing WHERE clause
                    table_name, col_name = date_columns[0]
                    where_pos = sql.upper().find('WHERE')
                    existing_where = sql[where_pos + 5:].strip()
                    
                    # Add year filter
                    year_filter = f"YEAR({col_name}) = {year}"
                    if existing_where:
                        new_where = f"WHERE {year_filter} AND ({existing_where})"
                    else:
                        new_where = f"WHERE {year_filter}"
                    
                    sql = sql[:where_pos] + new_where
                    print(f"      ‚úÖ Added missing year filter: YEAR({col_name}) = {year}")
                
                elif date_columns and 'WHERE' not in sql.upper():
                    # Add new WHERE clause
                    table_name, col_name = date_columns[0]
                    # Insert before ORDER BY, GROUP BY, or at the end
                    insert_pos = len(sql)
                    for keyword in ['ORDER BY', 'GROUP BY', 'HAVING']:
                        pos = sql.upper().find(keyword)
                        if pos != -1:
                            insert_pos = min(insert_pos, pos)
                    
                    year_filter = f" WHERE YEAR({col_name}) = {year}"
                    sql = sql[:insert_pos].rstrip() + year_filter + " " + sql[insert_pos:]
                    print(f"      ‚úÖ Added missing year filter: YEAR({col_name}) = {year}")
        
        # Ensure proper TOP limit
        if intent.get('aggregation') != 'count' and 'TOP' not in sql.upper():
            limit = intent.get('limit', 100)
            if sql.upper().startswith('SELECT'):
                sql = sql.replace('SELECT', f'SELECT TOP {limit}', 1)
        
        return sql
    
    def validate_sql(self, sql: str) -> bool:
        """Basic SQL validation"""
        if not sql:
            return False
        
        sql_upper = sql.upper().strip()
        
        # Must be SELECT
        if not sql_upper.startswith('SELECT'):
            return False
        
        # No dangerous operations
        dangerous = ['INSERT', 'UPDATE', 'DELETE', 'DROP', 'CREATE', 'ALTER']
        if any(op in sql_upper for op in dangerous):
            return False
        
        return True
    
    def generate_fallback_sql(self, tables: List[TableInfo], intent: Dict) -> str:
        """Generate simple fallback SQL with proper date filtering"""
        return self.generate_safe_fallback_sql(tables, intent)

class QueryExecutor:
    """Execution with retry - README Pattern A (Execution-Guided)"""
    
    def __init__(self, config: Config):
        self.config = config
        self.max_retries = 2
    
    async def execute_with_retry(self, sql: str, question: str, 
                               llm: SimpleRAGClient) -> Tuple[List[Dict], Optional[str]]:
        """Execute with execution-guided retry"""
        print("   üîÑ Stage 4b: Execution with retry...")
        
        for attempt in range(self.max_retries + 1):
            results, error = self.execute_sql(sql)
            
            if error is None:
                if len(results) == 0:
                    print(f"      ‚ö†Ô∏è Attempt {attempt + 1}: Empty results")
                    if attempt < self.max_retries:
                        sql = await self.retry_for_empty_results(sql, question, llm)
                        continue
                
                print(f"      ‚úÖ Success on attempt {attempt + 1}: {len(results)} rows")
                return results, None
            else:
                print(f"      ‚ö†Ô∏è Attempt {attempt + 1} failed: {error}")
                if attempt < self.max_retries:
                    sql = await self.retry_for_error(sql, error, question, llm)
                    continue
        
        return [], f"Failed after {self.max_retries + 1} attempts: {error}"
    
    def execute_sql(self, sql: str) -> Tuple[List[Dict], Optional[str]]:
        """Execute SQL with UTF-8 support"""
        if not sql.strip():
            return [], "Empty SQL query"
        
        try:
            with pyodbc.connect(self.config.get_database_connection_string()) as conn:
                # UTF-8 support for international characters
                conn.setdecoding(pyodbc.SQL_CHAR, encoding='utf-8')
                conn.setdecoding(pyodbc.SQL_WCHAR, encoding='utf-8')
                conn.setencoding(encoding='utf-8')
                
                cursor = conn.cursor()
                cursor.execute(sql)
                
                if cursor.description:
                    columns = [col[0] for col in cursor.description]
                    results = []
                    
                    for row in cursor:
                        row_dict = {}
                        for i, value in enumerate(row):
                            if i < len(columns):
                                row_dict[columns[i]] = safe_database_value(value)
                        results.append(row_dict)
                    
                    return results, None
                else:
                    return [], None
                    
        except Exception as e:
            return [], str(e)
    
    async def retry_for_error(self, failed_sql: str, error: str, 
                            question: str, llm: SimpleRAGClient) -> str:
        """Retry with error feedback"""
        
        retry_prompt = f"""
Previous SQL failed with error: {error}

FAILED SQL:
{failed_sql}

QUESTION: "{question}"

Generate corrected SQL that fixes the error.
Common fixes:
- Check column names exist
- Fix table names
- Correct JOIN syntax
- Fix WHERE conditions

Return only corrected SQL:
"""
        
        system_prompt = "You are an SQL error correction expert. Fix the SQL based on the error message."
        response = await llm.analyze(system_prompt, retry_prompt)
        
        return clean_sql_query(response)
    
    async def retry_for_empty_results(self, sql: str, question: str, llm: SimpleRAGClient) -> str:
        """Retry for empty results"""
        
        retry_prompt = f"""
Previous SQL returned 0 rows:

{sql}

QUESTION: "{question}"

Modify the SQL to be less restrictive and return some data:
- Remove strict WHERE conditions
- Use LEFT JOINs instead of INNER JOINs
- Check if date ranges are reasonable
- Consider broader criteria

Return modified SQL:
"""
        
        system_prompt = "You are an SQL optimization expert. Modify SQL to return meaningful results."
        response = await llm.analyze(system_prompt, retry_prompt)
        
        return clean_sql_query(response)

class QueryInterface:
    """Enhanced 4-Stage Pipeline with SQLGlot AST Validation - Following README exactly"""
    
    def __init__(self, config: Config):
        self.config = config
        self.llm = SimpleRAGClient(config)
        self.sql_generator = SQLGenerator(config)
        self.executor = QueryExecutor(config)
        
        # Show compliance with README requirements
        print("‚úÖ Enhanced Query Interface initialized with README compliance:")
        print(f"   üîí SQLGlot AST Validation: {'‚úÖ Available' if HAS_SQLGLOT else '‚ùå Not Available'}")
        print(f"   üìã View Patterns Loaded: {len(self.sql_generator.view_patterns)}")
        print(f"   üõ°Ô∏è Identifier Allowlist: {len(self.sql_generator.allowed_identifiers)} objects")
        print(f"   ‚ö° Constrained Generation: ‚úÖ Enabled")
        print(f"   üîÑ Execution-Guided Retry: ‚úÖ Enabled")
    
    async def start_session(self, tables: List[TableInfo], 
                          domain: Optional[BusinessDomain], 
                          relationships: List[Relationship]):
        """Start interactive session with enhanced 4-stage pipeline"""
        
        self.table_selector = TableSelector(tables)
        
        print(f"üöÄ Enhanced 4-Stage Pipeline Ready (Following README)")
        print(f"   üìä Tables: {len(tables)}")
        print(f"   üéØ View Patterns: {len(self.sql_generator.view_patterns)}")
        if domain:
            print(f"   üè¢ Domain: {domain.domain_type}")
        
        # Show some sample view patterns for transparency
        if self.sql_generator.view_patterns:
            print(f"   üí° Sample Business Patterns Available:")
            for pattern in self.sql_generator.view_patterns[:3]:
                business_type = pattern.get('business_pattern', 'general')
                confidence = pattern.get('confidence', 0.0)
                print(f"      ‚Ä¢ {business_type} (confidence: {confidence:.1f})")
        
        query_count = 0
        
        while True:
            try:
                question = input(f"\n‚ùì Enhanced Query #{query_count + 1}: ").strip()
                
                if question.lower() in ['quit', 'exit', 'q']:
                    break
                if not question:
                    continue
                
                query_count += 1
                print(f"üîÑ Processing with enhanced 4-stage pipeline...")
                
                start_time = time.time()
                result = await self.process_query(question)
                result.execution_time = time.time() - start_time
                
                self.display_result(result)
                
            except KeyboardInterrupt:
                print("\n‚è∏Ô∏è Interrupted")
                break
            except Exception as e:
                print(f"‚ùå Error: {e}")
    
    async def process_query(self, question: str) -> QueryResult:
        """4-Stage Pipeline Implementation"""
        
        try:
            # Stage 1: Intent Analysis (implicit in table selection)
            print("   üß† Stage 1: Intent analysis...")
            
            # Stage 2: Explainable Table Selection
            selected_tables, explanations = await self.table_selector.select_relevant_tables(
                question, self.llm
            )
            
            if not selected_tables:
                return QueryResult(
                    question=question,
                    sql_query="",
                    results=[],
                    error="No relevant tables found"
                )
            
            # Stage 3: Relationship Resolution (embedded in SQL generation)
            print("   üîó Stage 3: Relationship resolution...")
            
            # Stage 4: Validated SQL Generation + Execution
            sql = await self.sql_generator.generate_sql(
                question, selected_tables, explanations, self.llm
            )
            
            if not sql:
                return QueryResult(
                    question=question,
                    sql_query="",
                    results=[],
                    error="Failed to generate SQL"
                )
            
            # Execute with retry
            results, error = await self.executor.execute_with_retry(
                sql, question, self.llm
            )
            
            result = QueryResult(
                question=question,
                sql_query=sql,
                results=results,
                error=error,
                tables_used=[t.full_name for t in selected_tables]
            )
            
            # Add explanations
            result.explanations = explanations
            return result
            
        except Exception as e:
            return QueryResult(
                question=question,
                sql_query="",
                results=[],
                error=f"Pipeline error: {str(e)}"
            )
    
    def display_result(self, result: QueryResult):
        """Display results with enhanced explanations and validation info"""
        
        print(f"‚è±Ô∏è Completed in {result.execution_time:.1f}s")
        print("-" * 60)
        
        # Show explainable retrieval
        if hasattr(result, 'explanations'):
            explanations = result.explanations
            print(f"üìã EXPLAINABLE RETRIEVAL (README Pattern B):")
            print(f"   ‚Ä¢ Candidates: {explanations.get('total_candidates', 0)}")
            print(f"   ‚Ä¢ Lexical Matches: {len(explanations.get('lexical_matches', []))}")
            print(f"   ‚Ä¢ Selected: {len(explanations.get('selected_tables', []))}")
            
            for reason in explanations.get('reasoning', []):
                print(f"   ‚Ä¢ {reason}")
            print()
        
        # Show SQL generation details
        if result.error:
            print(f"‚ùå Error: {result.error}")
            if result.sql_query:
                print(f"üìã Generated SQL:")
                print(f"{result.sql_query}")
        else:
            print(f"üìã SQL Query (SQLGlot {'‚úÖ Validated' if HAS_SQLGLOT else '‚ö†Ô∏è Basic Validation'}):")
            print(f"{result.sql_query}")
            print(f"üìä Results: {len(result.results)} rows")
            
            if result.results:
                if result.is_single_value():
                    value = result.get_single_value()
                    column_name = list(result.results[0].keys())[0]
                    formatted_value = f"{value:,}" if isinstance(value, (int, float)) and value >= 1000 else str(value)
                    print(f"   üéØ {column_name}: {formatted_value}")
                else:
                    for i, row in enumerate(result.results[:5], 1):
                        display_row = {}
                        for key, value in list(row.items())[:6]:
                            if isinstance(value, str) and len(value) > 40:
                                display_row[key] = value[:37] + "..."
                            elif isinstance(value, (int, float)) and value >= 1000:
                                display_row[key] = f"{value:,}"
                            else:
                                display_row[key] = value
                        print(f"   {i}. {display_row}")
                    
                    if len(result.results) > 5:
                        print(f"   ... and {len(result.results) - 5} more rows")
            else:
                print("   ‚ö†Ô∏è No results returned")
        
        print("\nüí° Enhanced 4-Stage Pipeline Features (README Compliant):")
        print("   ‚úÖ Explainable table retrieval with business context")
        print("   ‚úÖ View-pattern analysis with proven business logic")
        print(f"   ‚úÖ {'SQLGlot AST validation' if HAS_SQLGLOT else 'Basic SQL validation'} with identifier allowlists")
        print("   ‚úÖ Constrained SQL generation with intent enhancement")
        print("   ‚úÖ Execution-guided retry with error recovery")
        print("   ‚úÖ UTF-8 international support with proper date filtering")